{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulte y presente el modelo y problema de optimización de los siguientes clasificadores\n",
    "\n",
    "-  Naive Bayes Gaussian NB\n",
    "- SGDClasffier\n",
    "- LogisticRegression\n",
    "- LinearDiscriminantAnalysis\n",
    "- KNeighborsClassifier\n",
    "- SVC\n",
    "- RandomForestClassifier\n",
    "-GuassianProcessClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Gaussian NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes es un clasificador probabilístico basado en el teorema de Bayes, que asume independencia condicional entre las características. GaussianNB es una variante que supone que los datos siguen una distribución normal (gaussiana). Como está basado en el teorema de bayes, que establece que la probabilidad de una clase \"C\", dato un conjunto de características X se expresa como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(C|X) = \\frac{P(X|C)P(C)}{P(X)}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "* $P(C)$ es la probabilidad previa de la clase.\n",
    "* $P(X|C)$ es la probabilidad condicional de los datos dado la clase.\n",
    "* $P(X)$ es la probabilidad marginal de los datos.\n",
    "\n",
    "En el caso de `GaussianNB`, asumimos que cada característica $x_i$ sigue una distribución normal:\n",
    "\n",
    "$$\n",
    "P(x_i|C) = \\frac{1}{\\sqrt{2\\pi\\sigma_C^2}} \\exp\\left(-\\frac{(x_i - \\mu_C)^2}{2\\sigma_C^2}\\right)\n",
    "$$\n",
    "\n",
    "Donde $\\mu_C$ y $\\sigma_C^2$ son la media y varianza de la característica $x_i$ en la clase $C$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema de optimización: Maximización de la verosimilitud**\n",
    "\n",
    "En Naïve Bayes, queremos encontrar los parámetros $\\mu_C$ y $\\sigma_C^2$ de cada clase $C$ que maximizan la función de verosimilitud:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{N} P(y_i|x_i, \\theta)\n",
    "$$\n",
    "\n",
    "Dado que asumimos independencia condicional y una distribución gaussiana para cada característica:\n",
    "\n",
    "$$\n",
    "P(x_i|C) = \\prod_{j=1}^{d} \\frac{1}{\\sqrt{2\\pi\\sigma_C^2}} \\exp\\left(-\\frac{(x_{ij} - \\mu_C)^2}{2\\sigma_C^2}\\right)\n",
    "$$\n",
    "\n",
    "Tomamos el logaritmo para obtener la log-verosimilitud:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\sum_{i=1}^{N} \\sum_{j=1}^{d} \\left[ -\\frac{1}{2} \\log(2\\pi\\sigma_C^2) - \\frac{(x_{ij} - \\mu_C)^2}{2\\sigma_C^2} \\right]\n",
    "$$\n",
    "\n",
    "La optimización consiste en derivar respecto a $\\mu_C$ y $\\sigma_C^2$ e igualar a cero:\n",
    "\n",
    "$$\n",
    "\\mu_C = \\frac{1}{N_C} \\sum_{i \\in C} x_i, \\quad \\sigma_C^2 = \\frac{1}{N_C} \\sum_{i \\in C} (x_i - \\mu_C)^2\n",
    "$$\n",
    "\n",
    "donde $N_C$ es el número de muestras en la clase $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATOS A TENER EN CUENTA: \n",
    "\n",
    "\n",
    "- La independencia entre caracteristicas rara vez se cumple en datos reales.\n",
    "- No maneja bien características correlacionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **SGDClassifier: Descenso de Gradiente Estocástico**\n",
    "\n",
    "## **¿Por qué se llama así?**\n",
    "El nombre **Descenso de Gradiente Estocástico (SGD)** proviene de:\n",
    "- **Descenso de Gradiente**: Método de optimización que ajusta los parámetros iterativamente en dirección opuesta al gradiente de la función de pérdida.\n",
    "- **Estocástico**: Indica que el gradiente se estima con una muestra o un minibatch en cada iteración, en lugar de usar todo el conjunto de datos.\n",
    "\n",
    "## **Modelo Matemático**\n",
    "SGD se usa para entrenar modelos lineales como:\n",
    "- **Regresión logística** (clasificación binaria).\n",
    "- **Máquinas de soporte vectorial (SVM)** (clasificación de margen máximo).\n",
    "\n",
    "Un **clasificador lineal** tiene la forma general:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(w^T x + b)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\hat{y} $ es la predicción (+1 o -1 en clasificación binaria).\n",
    "- \\( x \\) es el vector de características.\n",
    "- \\( w \\) es el vector de pesos.\n",
    "- \\( b \\) es el sesgo (bias).\n",
    "- $ \\text{sign}(\\cdot)$ asigna la clase según el signo de la combinación lineal.\n",
    "\n",
    "## **Problema de Optimización**\n",
    "El objetivo es minimizar una **función de pérdida**. Ejemplos:\n",
    "\n",
    "- **Regresión logística (log-loss):**\n",
    "\n",
    "$$\n",
    "L(w) = \\frac{1}{N} \\sum_{i=1}^{N} \\log(1 + e^{-y_i (w^T x_i + b)})\n",
    "$$\n",
    "\n",
    "- **Máquinas de soporte vectorial (hinge loss):**\n",
    "\n",
    "$$\n",
    "L(w) = \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, 1 - y_i (w^T x_i + b))\n",
    "$$\n",
    "\n",
    "- **Función de costo con regularización:**\n",
    "\n",
    "$$\n",
    "\\min_{w} \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, w^T x_i) + \\lambda R(w)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $ R(w) $ es un término de regularización $L2: \\frac{1}{2} \\|w\\|^2\\) o L1: \\|w\\|_1\\$.\n",
    "- $\\lambda $ controla la penalización de los pesos.\n",
    "## **Actualización de Parámetros con SGD**\n",
    "En cada iteración $ t $, se selecciona una muestra $(x_t, y_t) $ y se actualizan los pesos:\n",
    "\n",
    "$$\n",
    " w^{(t+1)} = w^{(t)} - \\eta \\nabla L(y_t, w^T x_t)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\eta$ es la tasa de aprendizaje (learning rate).\n",
    "- $\\nabla L(y_t, w^T x_t)$ es el gradiente de la función de pérdida.\n",
    "\n",
    "Este enfoque permite convergencia rápida en grandes conjuntos de datos.\n",
    "\n",
    "## **Conclusión**\n",
    "El **SGDClassifier** implementa esta estrategia para entrenar clasificadores lineales eficientemente en grandes volúmenes de datos, aprovechando el descenso de gradiente en muestras individuales para actualizar los pesos de forma iterativa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regresión Logística**\n",
    "\n",
    "La **Regresión Logística** es un modelo estadístico utilizado para problemas de clasificación binaria. En lugar de predecir directamente una clase, estima la **probabilidad** de que una observación pertenezca a una categoría.\n",
    "\n",
    "## **Modelo Matemático**\n",
    "Dado un conjunto de datos con características \\( x \\) y etiquetas \\( y \\in \\{0,1\\} \\), la regresión logística modela la probabilidad de que \\( y = 1 \\) como:\n",
    "\n",
    "$$\n",
    "P(y=1 | x) = \\sigma(w^T x + b)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $ \\sigma(z) $ es la **función sigmoide**, definida como:\n",
    "  \n",
    "  $$\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  $$\n",
    "  \n",
    "- $ w $ es el vector de pesos del modelo.\n",
    "- $ x $ es el vector de características de entrada.\n",
    "- $ b $ es el sesgo (bias).\n",
    "\n",
    "La decisión final se toma aplicando un umbral (por lo general, 0.5):\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\begin{cases} \n",
    "1, & \\text{si } P(y=1 | x) \\geq 0.5 \\\\\n",
    "0, & \\text{si } P(y=1 | x) < 0.5\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## **Problema de Optimización**\n",
    "El objetivo es encontrar los parámetros $ w $ y $ b $ que minimicen la **función de pérdida logarítmica (log-loss):**\n",
    "\n",
    "$$\n",
    "L(w) = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[y_i \\log(\\sigma(w^T x_i)) + (1 - y_i) \\log(1 - \\sigma(w^T x_i))\\right]\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $ N $ es el número total de muestras.\n",
    "- $ y_i $ es la etiqueta real de la muestra $ i $.\n",
    "- $ \\sigma(w^T x_i) $ es la probabilidad predicha de que $ y_i = 1 $.\n",
    "\n",
    "Para evitar sobreajuste, se añade un término de regularización \\( R(w) \\):\n",
    "\n",
    "$$\n",
    "\\min_{w} \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, w^T x_i) + \\lambda R(w)\n",
    "$$\n",
    "\n",
    "Donde $ \\lambda $ controla la penalización. Las opciones comunes son:\n",
    "- **L2 Regularización (Ridge):** $ R(w) = \\frac{1}{2} \\|w\\|^2 $\n",
    "- **L1 Regularización (Lasso):** $ R(w) = \\|w\\|_1 $\n",
    "\n",
    "## **Optimización con Descenso de Gradiente**\n",
    "Los parámetros se actualizan iterativamente usando el **gradiente descendente**:\n",
    "\n",
    "$$\n",
    " w^{(t+1)} = w^{(t)} - \\eta \\nabla L(w)\n",
    "$$\n",
    "\n",
    "Donde $ \\eta $ es la tasa de aprendizaje y el gradiente de la función de pérdida es:\n",
    "\n",
    "$$\n",
    "\\nabla L(w) = \\frac{1}{N} \\sum_{i=1}^{N} (\\sigma(w^T x_i) - y_i) x_i\n",
    "$$\n",
    "\n",
    "## **Conclusión**\n",
    "La **Regresión Logística** es un modelo poderoso para clasificación binaria, que utiliza la función sigmoide para predecir probabilidades y se optimiza con descenso de gradiente para ajustar los parámetros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA - LINEAR DISCRIMINANT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Análisis Discriminante Lineal (LDA) es un método de clasificación supervisado que encuentra una combinación lineal de características que maximiza la separación entre clases. Su objetivo es proyectar los datos en un espacio de menor dimensión mientras se conserva la mayor discriminación posible entre las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA es útil cuando se quiere:\n",
    "\n",
    "* Reducir la dimensionalidad de los datos (similar a PCA, pero considerando información de clase).\n",
    "* Maximizar la separabilidad entre diferentes clases en problemas de clasificación.\n",
    "* Mejorar la interpretabilidad de los datos en un espacio más pequeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelo Matemático**\n",
    "\n",
    "Dado un conjunto de datos con $ C $ clases, LDA busca encontrar una **proyección lineal** de los datos de entrada $ X $ en un nuevo espacio donde la separación entre clases sea máxima.\n",
    "\n",
    "Sea $ x \\in \\mathbb{R}^d $ un vector de características y $ y \\in \\{1, 2, ..., C\\} $ la etiqueta de clase, LDA modela la probabilidad condicional $ P(x | y) $ bajo la suposición de que:\n",
    "\n",
    "1. Las muestras de cada clase siguen una distribución normal multivariada.\n",
    "2. Todas las clases comparten la misma matriz de covarianza \\( \\Sigma \\).\n",
    "\n",
    "La función de decisión lineal de LDA se define como:\n",
    "\n",
    "$$\n",
    "\\delta_c(x) = x^T \\Sigma^{-1} \\mu_c - \\frac{1}{2} \\mu_c^T \\Sigma^{-1} \\mu_c + \\log P(y=c)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $ \\mu_c $ es el vector de medias de la clase $ c $.\n",
    "- $ \\Sigma $ es la matriz de covarianza compartida entre todas las clases.\n",
    "- $ P(y=c) $ es la probabilidad a priori de la clase $ c $.\n",
    "\n",
    "Un dato $ x $ se asigna a la clase con la mayor $ \\delta_c(x) $:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{c} \\delta_c(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Cálculo de los Parámetros**\n",
    "\n",
    "1. **Media de cada clase:**\n",
    "   $$\n",
    "   \\mu_c = \\frac{1}{N_c} \\sum_{i: y_i = c} x_i\n",
    "   $$\n",
    "   donde $ N_c $ es el número de muestras en la clase $ c $.\n",
    "\n",
    "2. **Matriz de covarianza compartida:**\n",
    "   $$\n",
    "   \\Sigma = \\frac{1}{N - C} \\sum_{c=1}^{C} \\sum_{i: y_i = c} (x_i - \\mu_c)(x_i - \\mu_c)^T\n",
    "   $$\n",
    "\n",
    "3. **Vectores discriminantes de Fisher:**  \n",
    "   Para reducir la dimensionalidad, LDA encuentra los **autovectores** de la matriz $ S_w^{-1} S_b $, donde:\n",
    "   - $ S_w $ es la matriz de dispersión intra-clase.\n",
    "   - $ S_b $ es la matriz de dispersión entre clases.\n",
    "\n",
    "   $$\n",
    "   S_w = \\sum_{c=1}^{C} \\sum_{i: y_i = c} (x_i - \\mu_c)(x_i - \\mu_c)^T\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   S_b = \\sum_{c=1}^{C} N_c (\\mu_c - \\mu)(\\mu_c - \\mu)^T\n",
    "   $$\n",
    "\n",
    "   donde $ \\mu $ es la media general de los datos.\n",
    "\n",
    "Los **autovectores** de $ S_w^{-1} S_b $ proporcionan las direcciones óptimas para la proyección de los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
